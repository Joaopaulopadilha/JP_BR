# bibliotecas/jpllama/jpllama.jp

# Importa a DLL que acabamos de compilar
nativo "bibliotecas/jpllama/jpllama.jpd" importar llama_init(0), llama_carregar(1), llama_prompt(2), llama_gerar_token(1), llama_liberar(1), llama_versao(0)

# Inicializa o backend globalmente (carrega drivers da CPU)
llama_init()

classe Llama:
    ptr_modelo: 0
    
    # Carrega o arquivo .gguf
    funcao carregar(caminho):
        saida("Tentando carregar: " + caminho)
        ptr_modelo = llama_carregar(caminho)
        
        se ptr_modelo != 0:
            saida("Modelo carregado com sucesso! Endereco memoria: " + ptr_modelo)
            retorna verdadeiro
        senao:
            saida("FALHA CRITICA: Nao foi possivel carregar o modelo.")
            retorna falso

    # Envia o texto inicial para a IA "ler"
    funcao processar(prompt):
        se ptr_modelo == 0:
            retorna falso
        
        llama_prompt(ptr_modelo, prompt)
        retorna verdadeiro

    # Gera o proximo pedaco de texto
    funcao gerar():
        se ptr_modelo == 0:
            retorna ""
            
        token = llama_gerar_token(ptr_modelo)
        retorna token

    # Limpa a memoria RAM
    funcao liberar():
        se ptr_modelo != 0:
            llama_liberar(ptr_modelo)
            ptr_modelo = 0

# Cria uma instancia global para facilitar o uso
ia = novo Llama()